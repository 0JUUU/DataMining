# -*- coding: utf-8 -*-
"""WindClassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zOtnejVD8dKaBNFUBNRXIayDVmegXAG8
"""

from google.colab import drive
drive.mount('/gdrive')

"""# 와인 감별사 : 와인의 Quality를 분류하는 Classifier 만들기

## 1. 과제 설명
이번 과제에서는 케라스(Keras)를 활용하여, 와인의 품질을 분류하는 인공신경망 분류기를 만들어 볼 것입니다.
케라스는 Tensorflow, Theano 등의 딥 러닝 라이브러리 위에서 동작하는 오픈 소스 라이브러리로, 보다 쉬운 API를 제공함으로써 모델 설계 및 학습, 테스트가 간단하다는 장점이 있습니다. 

### 1.1 케라스 설치를 위한 필수 라이브러리
케라스를 설치하기 전에 먼저 필수적으로 설치해야 할 것들이 있습니다.
* Anaconda : Python 3.x 버전, Numpy, Pandas, SciPy, sklearn 등 필수 라이브러리들이 포함된 통합 배포 팩
<br> 아나콘다 설치 : https://www.anaconda.com/distribution/#download-section
* Tensorflow : Google에서 개발한 오픈 소스 딥 러닝 라이브러리. <b>설치된 Python 버전과 호환되는 것으로 설치할것!</b>
<br> 텐서플로우 설치 : https://www.tensorflow.org/install/pip
<br> * CPU 버전을 설치할 것을 권장. 

### 1.2 케라스 설치
위 라이브러리들을 설치한 후, 케라스를 설치합니다.
* https://keras.io/#installation

### 1.3 케라스 설치 확인
케라스가 올바르게 설치되었는지 확인하기 위해, 케라스를 Import한 뒤 버전을 출력해봅니다.
"""

import keras

keras.__version__

"""위와 같이 케라스의 버전이 출력되면 정상입니다. (출력되는 버전은 위 예시와 다를 수도 있음)<br> 나중에 신경망을 만들기 위한 클래스들도 함께 Import 합시다."""

from keras import Sequential
from keras.layers import Dense, Activation

"""---
## 2. Data Set 설명
 본 과제에서 사용할 데이터 셋은 UCI에서 제공되는 Wine Quality Data Set입니다. (https://archive.ics.uci.edu/ml/datasets/Wine+Quality) 데이터는 레드 와인 1599개, 화이트 와인 4898개의 화학적 특성을 포함하고 있습니다. 데이터는 두 개의 CSV(Comma-seperated values)형태로 제공되며, 구성은 다음과 같습니다.
* 화이트 와인 / 레드 와인 CSV 파일
* 11개의 실수(Real) 입력 변수 (X)
    * fixed acidity
    * volatile acidity
    * citric acid
    * residual sugar
    * chlorides
    * free sulfur dioxide
    * total sulfur dioxide
    * density
    * pH
    * sulphates
    * alcohol
* 1개의 클래스 레이블 (Y)
   * quality (0~10, 0: Very poor, 10: Very excellent)
* Missing Value 없음
* 클래스들이 불균등하게 분포함.

더 자세한 사항은 블랙보드에 함께 올라가있는 설명 파일을 참고하도록 합시다.

### 2.1 데이터 로드
데이터 분석에서 가장 많이 사용되는 라이브러리 중 하나인 Pandas와 Numpy를 Import하겠습니다. Pandas는 데이터 분석에 유용한 데이터 타입인 DataFrame을 제공하며, Numpy는 효율적이고 빠른 매트릭스 연산을 지원합니다.
"""

import pandas as pd
pd.__version__
pd.options.display.max_rows=15

import numpy as np
np.__version__

"""Pandas를 이용해서 CSV 파일을 읽어들이도록 합시다. white_wine 변수에는 화이트 와인 데이터를, red_wine 변수에는 레드 와인 데이터를 읽어들입니다."""

#########################코드########################


white_wine = pd.read_csv('/gdrive/My Drive/winequality-white.csv', header = 'infer')
red_wine = pd.read_csv('/gdrive/My Drive/winequality-red.csv', header = 'infer')


#####################################################

"""### 2.2 데이터 전처리
데이터를 읽어들인 뒤, 읽어들인 데이터프레임을 display 함수를 통해 확인합니다.
"""

display(white_wine)

display(red_wine)

"""이제 데이터프레임을 입력 변수와 정답 셋(클래스 레이블)으로 나누는 함수를 작성하겠습니다.<br>
<b>generate_data</b>함수는 데이터프레임 객체와 테스트 셋 비율을 입력으로 받아, 네 개의 numpy array를 반환합니다. 트레이닝 셋과 테스트 셋의 비율은 training_set_ratio에 의해 결정됩니다.
* Function : generate_data
 * 입력
     * pd.DataFrame : df
     * double : training_set_ratio  
 * 출력
     * np.array : X_train
     * np.array : Y_train
     * np.array : X_test
     * np.array : Y_test
"""

#####################################################
from sklearn.model_selection import train_test_split
def generate_data(df, t_r):
    X = df.drop(['quality'],axis = 1)
    Y = df['quality']
    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, train_size = t_r)

    return X_train.values, Y_train, X_test.values, Y_test
#####################################################

x_train, y_train, x_test, y_test = generate_data(white_wine, 0.7)

"""작성한 함수를 호출하여 화이트 와인 데이터에 대해 트레이닝 셋과 테스트 셋의 입력과 정답이 적절하게 생성되었는지 확인합니다.

---

# 3. 케라스를 이용한 모델 생성, 학습, 테스트
입력 데이터와 정답 셋이 만들어졌으니 케라스를 사용하여 각 데이터에 대한 분류기를 생성하고, 트레이닝 셋으로 학습시킨 뒤 테스트 정확도를 관찰합니다.

# 과제
### 1. 화이트 와인 분류 모델과 레드 와인 분류 모델 설계 및 학습
* 하나의 히든 레이어에 32개의 노드를 가진 인공신경망 모델 생성 및 모델 학습
* 트레이닝 Epoch에 따라 Loss의 변화를 그래프로 시각화
* 테스트 셋에 대한 정확도 기록
"""

# Commented out IPython magic to ensure Python compatibility.
##########################################################

# %matplotlib inline
import matplotlib.pyplot as plt

# white wine
whitemodel = Sequential()
whitemodel.add(Dense(32, input_dim = 11, activation='relu'))
whitemodel.add(Dense(units=11, activation='softmax'))

whitemodel.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])


###########################################################

hist = whitemodel.fit(x_train, y_train, validation_data=[x_test, y_test], epochs=100, batch_size=10)

train_loss = hist.history['loss']
test_loss = hist.history['val_loss']
plt.title('White Wine loss')
plt.plot(train_loss, 'b', label='train loss')
plt.plot(test_loss, 'r', label='test loss')
plt.legend(loc='best')
plt.grid()
plt.xlabel('epoch')
plt.ylabel('loss')

plt.show()

train_acc = hist.history['accuracy']
test_acc = hist.history['val_accuracy']

plt.title('White Wine Acc')
plt.plot(train_acc, 'g', label='train acc')
plt.plot(test_acc, 'b', label='test acc')
plt.legend(loc='best')
plt.grid()
plt.xlabel('epoch')
plt.ylabel('acc')

plt.show()

test_loss, test_acc = whitemodel.evaluate(x_test, y_test,verbose = 2)
print(test_acc)

#red wine
x_train, y_train, x_test, y_test = generate_data(red_wine, 0.7)

redmodel = Sequential()
redmodel.add(Dense(32, input_dim = 11, activation='relu'))
redmodel.add(Dense(units=11, activation='softmax'))

redmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

hist = redmodel.fit(x_train, y_train, validation_data=[x_test, y_test], epochs=100, batch_size=10)

train_loss = hist.history['loss']
test_loss = hist.history['val_loss']
plt.title('Red Wine loss')
plt.plot(train_loss, 'b', label='train loss')
plt.plot(test_loss, 'r', label='test loss')
plt.legend(loc='best')
plt.grid()
plt.xlabel('epoch')
plt.ylabel('loss')

plt.show()

test_loss, test_acc = redmodel.evaluate(x_test, y_test, verbose=2)
print(test_acc)

"""### 2. 각 모델의 성능을 향상시킬 수 있는 방법 적용
* 하이퍼파라미터를 변경하여 테스트 셋에서의 정확도를 향상시킬 것
    * 예) 레이어 수, 노드 수, Learning rate 등
* 하이퍼파라미터를 변화시킨 각각의 모델에 대해, 트레이닝 Epoch 당 Loss의 변화를 기록하고 이를 시각화
* 그 외 성능을 향상시킬 수 있는 모든 방법을 사용하여 가장 성능이 좋은 모델을 선택
    * 예) Dropout, Normalization 등
"""

# Commented out IPython magic to ensure Python compatibility.
##########################################################
from keras import optimizers

# white wine
x_train, y_train, x_test, y_test = generate_data(white_wine, 0.7)
##########################################################
#Before improvement
# %matplotlib inline
import matplotlib.pyplot as plt

# white wine
whitemodel = Sequential()
whitemodel.add(Dense(32, input_dim = 11, activation='relu'))
whitemodel.add(Dense(units=11, activation='softmax'))

whitemodel.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

hist = whitemodel.fit(x_train, y_train,validation_data = [x_test, y_test], epochs=100, batch_size=10)

train_loss = hist.history['loss']
test_loss = hist.history['val_loss']
plt.title('White Wine loss')
plt.plot(train_loss, 'b', label='train loss')
plt.plot(test_loss, 'r', label='test loss')
plt.legend(loc='best')
plt.grid()
plt.xlabel('epoch')
plt.ylabel('loss')

plt.show()


result = whitemodel.evaluate(x_test, y_test, verbose=2)
print("Before Improvement : test_Accuracy = "+str(result[1]))
print("")
###########################################################

#improvement
from keras.callbacks import EarlyStopping
from keras.optimizers import Adam
from keras.layers import  Dropout
from keras.layers import Activation
from keras.layers.normalization import BatchNormalization

model = Sequential()
model.add(Dense(64, input_dim = 11))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dense(64))
model.add(BatchNormalization()) 
model.add(Activation('relu'))
model.add(Dense(units=11, activation='softmax'))

opt = optimizers.Adam(learning_rate=0.002)
model.compile(loss = 'sparse_categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])

ES = EarlyStopping(monitor='val_loss', patience=25)
hist = model.fit(x_train, y_train, validation_data = [x_test, y_test], epochs=50, batch_size=256, verbose=2)

train_loss = hist.history['loss']
test_loss = hist.history['val_loss']
plt.title('Improve White Wine loss')
plt.plot(train_loss, 'b', label='train loss')
plt.plot(test_loss, 'r', label='test loss')
plt.legend(loc='best')
plt.grid()
plt.xlabel('epoch')
plt.ylabel('loss')

plt.show()


result = model.evaluate(x_test, y_test, verbose=2)
print("After Improvement : test_Accuracy = "+str(result[1]))
print("")
###########################################################

"""### 3. 화이트 와인과 레드 와인을 하나의 모델만 사용하여 분류
* 화이트 와인과 레드 와인 데이터를 합쳐 wine 데이터 셋 생성
* 입력이 화이트 와인인지 레드 와인인지에 관계없이 와인 품질을 분류하는 모델 생성
* 모델의 성능을 향상시킬 수 있는 방법을 찾아 적용할 것
"""

##########################################################
from pandas import DataFrame
ww = pd.DataFrame(white_wine)
rw = pd.DataFrame(red_wine)

wine = pd.concat([rw,ww], ignore_index = True)
display(wine)

###########################################################

x_train, y_train, x_test, y_test = generate_data(wine,0.7)

# wine
model = Sequential()
model.add(Dense(32, input_dim = 11, activation='relu'))
model.add(Dense(units=11, activation='softmax'))

model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

hist = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs=100, batch_size=10)

train_loss = hist.history['loss']
test_loss = hist.history['val_loss']
plt.title('Wine loss')
plt.plot(train_loss, 'b', label='train loss')
plt.plot(test_loss, 'r', label='test loss')
plt.legend(loc='best')
plt.grid()
plt.xlabel('epoch')
plt.ylabel('loss')

plt.show()

test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
print(test_acc)